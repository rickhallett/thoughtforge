üìö ThoughtForge

Welcome to ThoughtForge. This app is designed to automate and streamline the process of transforming raw content into polished articles ready for publishing across multiple channels. The initial focus is on developing a robust end-to-end pipeline as a proof of concept, but the goal is to eventually convert this into a scalable SaaS platform.

üìö Documentation

[Backend Roadmap](docs/backend-roadmap.md) (est. 1-2 weeks FTE 1.0)
[DB Design](docs/db-design.md)
[Frontend Roadmap](docs/frontend-roadmap.md) (est. 1-2 weeks FTE 1.0)
[AI Roadmap](docs/ai-roadmap.md) (est. 4-6 weeks FTE 1.0)
[SaaS Roadmap](docs/saas-roadmap.md) (est. 2-3 months FTE 1.0)

üöÄ Project Overview

	‚Ä¢	Tech Stack: TypeScript, Node.js, Express, Prisma, Postgres for application data and state, Redis for queuing and job processing
	‚Ä¢	Design Philosophy: Minimal library usage, coding from first principles for a lightweight and fast application
	‚Ä¢	Development Approach: Test-Driven Development (TDD) with Jest for unit and integration tests, moving towards end-to-end (E2E) testing
	‚Ä¢	Deployment: Hosted on AWS Elastic Beanstalk, Postgres also hosted on AWS

üåü Features

1. Content Ingestion Endpoint

An API endpoint that accepts input sources with optional query parameters:
	‚Ä¢	Query Parameters:
    ‚Ä¢	publishingDestinations: Target platforms for publishing
    ‚Ä¢	minBlogLength & maxBlogLength: Desired length range of the blog
    ‚Ä¢	style: Writing style preferences
    ‚Ä¢	tone: Tone of the content
    ‚Ä¢	educationLevel: Target audience‚Äôs education level
    ‚Ä¢	blogContentType: Type of content (e.g., thought leadership, technical, clinical)
    ‚Ä¢	llmEnhancements: Desired AI enhancements (e.g., generate summary, extract keywords)
    ‚Ä¢	Input Format: Minimally structured Markdown file with:
    ‚Ä¢	Title (optional): If absent, a title will be generated in the pipeline
    ‚Ä¢	Focus Points (optional)
    ‚Ä¢	Body: Main content
    ‚Ä¢	Tags (optional)

2. Original Source Storage

	‚Ä¢	Each input is assigned a unique ID.
	‚Ä¢	Stored in the original_source database table for tracking and reference.

3. Content Processing Pipeline

The pipeline is divided into three queues:
	1.	Content Standardization: Normalizes the content structure.
	2.	Content Processing: Enhances content through multiple stages.
	3.	Content Publishing: Prepares content for release.

	‚Ä¢	Transformation Tracking: A database table logs content at various transformation stages.

4. Content Processing Stages

	‚Ä¢	AI Enhancement:
	‚Ä¢	Dynamically composed prompts based on query parameters (tone, style).
	‚Ä¢	AI operations initially handled by OpenAI‚Äôs remote services.
	‚Ä¢	SEO Enhancement: Optimizes content for search engines.
	‚Ä¢	Approval Stage: Content is held for personal review and approval before publishing.

5. Publishing Workflow

	‚Ä¢	Post-Approval Scheduling:
    ‚Ä¢	Approved content moves to the publishing queue.
    ‚Ä¢	Initially schedules one article per day.
    ‚Ä¢	Future enhancements will manage custom delivery across multiple channels based on queue length and    channel feedback.
	‚Ä¢	API Access:
    ‚Ä¢	Retrieve articles in the review stage by ID and title.
    ‚Ä¢	Future search functionality to query by keywords, subjects, tags, etc.

6. Notifications and Logging

	‚Ä¢	Email Notifications: Receive emails upon successful publication of articles.
	‚Ä¢	Error Logging: Comprehensive logs for any failures during processing stages.

7. Pipeline Statistics API

	‚Ä¢	An endpoint to retrieve statistics across the entire pipeline for monitoring and analytics.

8. Deployment and Scalability

	‚Ä¢	AWS Deployment: Hosted on AWS Elastic Beanstalk for scalability and reliability.
	‚Ä¢	Lightweight Architecture: Minimal dependencies to ensure high performance.

9. Testing Strategy

	‚Ä¢	Test Coverage: Comprehensive unit and integration tests using Jest.
	‚Ä¢	Development Methodology: Primarily Test-Driven Development (TDD).
	‚Ä¢	Future Testing: Plans to implement E2E testing to cover the entire application flow.

üõ†Ô∏è Future Development Plans

Front-End Application

	‚Ä¢	Tech Stack: TypeScript, React, Tailwind CSS
	‚Ä¢	Deployment: Likely on Vercel or AWS
	‚Ä¢	Features:
    ‚Ä¢	Full CRUD operations
    ‚Ä¢	Drag-and-drop file uploads
    ‚Ä¢	Multi-document uploads
    ‚Ä¢	Web clipping via a separate web scraper
    ‚Ä¢	Rich text editor
    ‚Ä¢	Scheduling options with a calendar view
    ‚Ä¢	Manual schedule editing and release parameter profiles
    ‚Ä¢	Mobile-first design for on-the-go usage
    ‚Ä¢	Voice note capturing for real-time idea addition
    ‚Ä¢	Testing:
    ‚Ä¢	TDD using React Testing Library
    ‚Ä¢	E2E testing for the front-end application

Custom LLM Integration

	‚Ä¢	Objective: Host a custom open-source Large Language Model (LLM) fine-tuned on personal writing style.
	‚Ä¢	Hosting: Google Cloud Platform
	‚Ä¢	Technologies:
	‚Ä¢	Agentic workflows with Python LangChain framework
	‚Ä¢	Inspection with LangGraph
	‚Ä¢	Future LLM Instances:
	‚Ä¢	A separate LLM for clinical blog creation, fine-tuned on anonymized client data.

Multi-Media Content Conversion

	‚Ä¢	Expansion: Support for various input and output types.
	‚Ä¢	Goal: Enable full multimedia conversion into blogs and other content forms.

üìà Project Status

	‚Ä¢	Current Phase: Developing the first end-to-end pipeline as a proof of concept.
	‚Ä¢	Test Coverage: High coverage with unit and integration tests. E2E tests to be added in future stages.

üì¨ Get Involved

	‚Ä¢	Contributions: Feel free to fork the repository and submit pull requests.
	‚Ä¢	Issues: Report bugs or request features via the GitHub Issues tab.

üìù License

This project is open-source and available under the MIT License.

Developed with ‚ù§Ô∏è and TypeScript.

üéØ Getting Started

Prerequisites

	‚Ä¢	Node.js (v14 or above)
	‚Ä¢	npm or yarn
	‚Ä¢	AWS Account (for deployment)
	‚Ä¢	OpenAI API Key (for AI enhancements)

üöÄ Installation

	1.	Clone the Repository

git clone https://github.com/yourusername/content-pipeline-project.git


	2.	Install Dependencies

cd content-pipeline-project
npm install
# or
yarn install


	3.	Set Up Environment Variables
Create a .env file in the root directory and add your configuration:

OPENAI_API_KEY=your_openai_api_key
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
DATABASE_URL=your_database_connection_string


	4.	Run Tests

npm test
# or
yarn test


	5.	Start the Application

npm start
# or
yarn start



üìö Documentation

	‚Ä¢	API Endpoints: Detailed API documentation is available in the API Docs.
	‚Ä¢	Developer Guide: For contributing guidelines and development setup, refer to the Developer Guide.

üìä Test Coverage

	‚Ä¢	Unit Tests: Comprehensive coverage with Jest.
	‚Ä¢	Integration Tests: Key integration points tested.
	‚Ä¢	Coverage Report: Available in the coverage directory after running tests.

Feel free to explore, contribute, and provide feedback to help improve the Content Pipeline Project!

üôè Acknowledgments

	‚Ä¢	OpenAI: For providing powerful AI capabilities.
	‚Ä¢	AWS: For scalable deployment solutions.
	‚Ä¢	Community Contributors: Thank you to everyone who has contributed to this project.

üì´ Contact

	‚Ä¢	Email: your.email@example.com
	‚Ä¢	GitHub: yourusername

Last updated on {{current_date}}.